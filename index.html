<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>IMO Bench</title>
<style>
    body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji";
        line-height: 1.6;
        color: #24292e;
        max-width: 900px;
        margin: 0 auto;
        padding: 20px;
    }
    h1, h2, h3 {
        border-bottom: 1px solid #eaecef;
        padding-bottom: .3em;
        margin-top: 24px;
    }
    p {
        margin-top: 0;
        margin-bottom: 16px;
    }
    a {
        color: #0366d6;
        text-decoration: none;
    }
    a:hover {
        text-decoration: underline;
    }
    blockquote {
        margin: 0 0 16px 0;
        padding: 0 1em;
        color: #6a737d;
        border-left: 0.25em solid #dfe2e5;
    }
    pre {
        padding: 16px;
        overflow: auto;
        font-size: 85%;
        line-height: 1.45;
        background-color: #f6f8fa;
        border-radius: 6px;
    }
    code {
        font-family: SFMono-Regular, Consolas, "Liberation Mono", Menlo, monospace;
    }
    ul {
        padding-left: 2em;
        margin-top: 0;
        margin-bottom: 16px;
    }
    img {
        max-width: 100%;
    }
    header-logo {
        /* Set a fixed height as defined in your HTML (height="50") */
        height: 50px; 
        
        /* Crucial for scaling: ensures the width adjusts automatically */
        width: auto; 
        
        /* Optional: Basic display settings for cleaner integration */
        display: block; 
        /* margin: 0;  */
        margin-right: auto;
        margin-left: 0;
    }

    /* --- New Button Styling --- */
    .action-button {
        display: inline-flex;
        align-items: center;
        gap: 8px; /* Space between icon and text */
        padding: 10px 18px;
        font-weight: 600;
        font-size: 16px;
        line-height: 1;
        text-decoration: none; /* Important to remove default link underline */
        color: #fff;
        background-color: #2c3e50; /* Dark Blue/Gray */
        border: none;
        border-radius: 10px; /* Nicely rounded corners */
        transition: background-color 0.15s ease, box-shadow 0.15s ease;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        cursor: pointer;
    }

    .action-button:hover {
        background-color: #34495e; /* Slightly darker on hover */
        box-shadow: 0 6px 10px rgba(0, 0, 0, 0.15);
        text-decoration: none; /* Ensure no underline on hover either */
    }

    .action-button i {
        font-size: 1.1em; /* Adjust icon size */
    }

</style>
<!-- Load Phosphor Icons for the buttons -->
<script src="https://unpkg.com/@phosphor-icons/web@2.0.3/dist/assets/index.js"></script>
</head>
<body>

<div>
  <img src="imgs/Google_DeepMind_Logo_rgb_3320x512px.png" 
       height="50" 
       class="header-logo">
</div>
    
<h1>Towards Robust Mathematical Reasoning</h1>

<!-- AUTHORS -->
<div class="text-lg text-gray-700 mb-8">
    <!-- Author Row 1 -->
    <p class="mb-2">
        Thang Luong<span class="superscript-symbol">â—Š</span>,
        Dawsen Hwang<span class="superscript-symbol">*</span>,
        Hoang Nguyen<span class="superscript-symbol">*â€ </span>,
        Golnaz Ghiasi<span class="superscript-symbol">*</span>,
        Yuri Chervonyi<span class="superscript-symbol">*</span>,
        Insuk Seo<span class="superscript-symbol">*</span>,
        Junsu Kim<span class="superscript-symbol">*</span>,
	<br>
        Garrett Bingham,
        Jonathan Lee,
        Swaroop Mishra<span class="superscript-symbol">â€ </span>,
        Alex Zhai,
        Clara Huiyi Hu,
        Henryk Michalewski,
	<br>
        Jimin Kim<span class="superscript-symbol">â€ </span>,
        Jeonghyun Ahn<span class="superscript-symbol">â€ </span>,
        Junhwi Bae<span class="superscript-symbol">â€ </span>,
        Xingyou Song,
        Trieu H. Trinh,
        Quoc V. Le,
        Junehyuk Jung<span class="superscript-symbol">â—Š</span>
    </p>
</div>

<!-- CORRESPONDENCE AND AFFILIATIONS -->
<div class="text-md text-gray-600 space-y-2 mb-16 mx-auto w-fit text-left">
    <div class="flex items-start">
        <span class="affiliation-symbol">â—ŠCorresponding authors: thangluong@google.com, junehuyk@google.com</span>
    </div>
    <div class="flex items-start">
        <span class="affiliation-symbol">*Core and equal contributors</span>
        <span class="affiliation-symbol">â€ Work previously conducted under Google DeepMind.</span>
    </div>
</div>

<!-- NAVIGATION BUTTONS (Footer Area) -->
<div style="display: flex; flex-wrap: wrap; justify-content: flex-start; gap: 16px; margin-bottom: 24px;">
    
    <!-- Button 1: Paper -->
    <a href="https://aclanthology.org/2025.emnlp-main.1794/" class="action-button">
        <i class="ph ph-file-text"></i>
        ðŸ“„ Paper
    </a>

    <!-- Button 2: Dataset -->
    <a href="https://github.com/google-deepmind/superhuman/tree/main/imobench" class="action-button">
        <i class="ph ph-git-branch"></i>
        ðŸ¥‡ Dataset
    </a>
</div>

<h2>Introduction</h2>
<p>In July 2025, we celebrated a historic <a href="https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/">achievement</a>: our advanced Gemini model with Deep Think technology reached a gold-medal standard at the International Mathematical Olympiad (IMO). This was a landmark moment for AI. But the win wasn't just about getting strong performances at IMO problems - it was about building a system capable of deep, robust mathematical reasoning.</p>

<p>At this year <a href="https://2025.emnlp.org/">EMNLP 2025</a>, we are excited to introduce IMO-Bench, a suite of advanced reasoning benchmarks that played a crucial role in our IMO-gold journey and were designed to push the frontiers of mathematical reasoning in AI. Vetted by a panel of IMO medalists and mathematicians, IMO-Bench specifically targets the level of IMO due to its notoriously difficult problems, which require both rigorous multi-step reasoning and creativity beyond simple application of known formulas. IMO-Bench consists of three benchmarks that judge models on diverse capabilities: <em>IMO-AnswerBench</em> - a large-scale test on getting the right answer, <em>IMO-ProofBench</em> - a next-level evaluation for proof writing, and <em>IMO-GradingBench</em> to enable further progress in automatic evaluation of long-form answers.</p>

<p>A key highlight of our work is to show that autograders built with Gemini reasoning correlate well with human evaluations on IMO-ProofBench, as illustrated below, for a wide range of foundation models. This was achieved thanks to the accompanying grading schemes in IMO-Bench, which are suitable for both human experts and automated systems. We ultimately hope to steer the community's focus from final answers to the proofs themselves, enabling a more rigorous assessment of AI reasoning processes. We <a href="https://github.com/google-deepmind/superhuman/tree/main/imobench/">release</a> IMO-Bench together with rich grading data to the community to help enable further advancements towards robust mathematical reasoning.</p>

<div style="text-align: center;">
</div>

<div style="text-align: center;">

  <img src="imgs/Main-Plot.png" alt="Correlation between human and automatic evaluations on IMO-ProofBench" height="500">

</div>

<blockquote>
<p>IMO-ProofBench, a benchmark in IMO-Bench, for measuring proof-writing capabilities. We demonstrated high correlations between human and automatic evaluations on a variety of public models, including our IMO-gold model.</p>
</blockquote>

<h2>IMO-AnswerBench</h2>

<p>The first benchmark, IMO-AnswerBench, consists of 400 problems with verifiable answers carefully chosen from past Olympiad competitions. The problems span across four IMO categories (Algebra, Combinatorics, Geometry, and Number Theory) and are altered by experts to avoid memorization. For each category, the benchmark contains 100 problems across four levels of difficulty: pre-IMO (middle school or pre-Math Olympiad problems), IMO-Easy (equivalent to Problem 1 or Problem 4 at the IMO), IMO-Medium (equivalent to Problem 2 or Problem 5 at the IMO) and IMO-Hard (equivalent to Problem 3 or Problem 6 at the IMO or post-Math Olympiad problems). Problems were chosen from a variety of topics whose solutions require different problem solving techniques to ensure a diverse representation of topics, ideas, and domain knowledge as illustrated below.</p>

<div style="text-align: center;">
  <img src="imgs/imo_answer_bench.png" alt="IMO-AnswerBench Topic Distribution" height="500">
</div>

<blockquote>
<p>Topic distribution by category in IMO-AnswerBench. Number Theory and Combinatorics have the most topics which reflect the broad knowledge required to solve these problems while Geometry is mostly skewed towards angle and sidelength computation problems due to the nature of the short answer benchmark.</p>
</blockquote>

<p>Our Gemini Deep Think (IMO Gold) model achieved an overall accuracy of 80.0%, surpassing the best non-Gemini model (Grok 4) by 6.9% and the best open-weight model (DeepSeek R1) by 19.2%. Latest models such as GPT-5 are still struggling with overall accuracy of only 65.6% respectively. We show breakdowns of performances across the four categories (Algebra, Combinatorics, Geometry, Number Theory) in the figure below. Generally, models perform the worst in Combinatorics, potentially highlighting difficulties with advanced abstract reasoning.</p>

<div style="text-align: center;">
  <img src="imgs/model_accuracy_answerbench.png" alt="Model Accuracy on IMO-AnswerBench">
</div>

<blockquote>
<p>Model accuracy on IMO-AnswerBench. Results are averaged over 8 runs, except for Gemini 2.5 Deep Think and Gemini Deep Think (IMO Gold) (single run).</p>
</blockquote>

<h2>IMO-ProofBench</h2>

<p>While the final answer accuracy provided by IMO-AnswerBench offers a valuable metric for measuring mathematical abilities, it is insufficient for a comprehensive assessment of mathematical reasoning. We, hence, introduce IMO-ProofBench as the next-level evaluation designed to evaluate the ability of AI models in constructing rigorous and valid mathematical arguments. With 60 proof-based problems, the benchmark is divided into two subsets: a basic set covering pre-IMO to IMO-Medium difficulty levels, and an advanced set featuring novel, highly challenging problems simulating complete IMO examinations, up to IMO-Hard level. Our goal for the basic set is to assess models in their early stages of development. Sufficiently strong performance on the basic set would justify progression to the advanced set.</p>

<p>Performances on the basic IMO-ProofBench varies significantly: while Gemini Deep Think (IMO Gold) achieves a high score of 89.0%, most models score below 60%, indicating that there is still considerable room for improvements. The advanced IMO-ProofBench proves to be a more significant challenge that all non-Gemini models score below 25%. Our Gemini Deep Think (IMO Gold) model achieved a state-of-the art score of 65.7% according to human evaluations. This represents a substantial leap in capability, but its distance from a perfect score indicates that even the strongest models have room for growth in sophisticated mathematical reasoning.</p>

<div style="text-align: center;">
  <img src="imgs/model_accuracy_proofbench.png" alt="Expert evaluation on IMO-ProofBench">
</div>

<blockquote>
<p>Expert evaluation results on the Basic and Advanced subsets of IMO-ProofBench. Scores are presented as a percentage of the total possible points, with each problem graded from 0-7.</p>
</blockquote>

<h3>Proof Autograder</h3>

<p>While human expert evaluation remains the gold standard for mathematical proofs, its cost and time intensity limit scalable research. To address this, we built ProofAutoGrader, an automatic grader for IMO-ProofBench. The autograder leverages Gemini 2.5 Pro, providing it with a prompt containing the problem statement, the candidate solution, a reference solution, and specific grading guidelines.</p>

<p>When applying ProofAutoGrader to the 14 public models on IMO-ProofBench, the average grades from our automatic grader strongly correlate with human grades, yielding high Pearson correlation coefficients of 0.96 and 0.93 on both basic and advanced problems respectively. In addition, we also tested ProofAutoGrader on 170 internal systems, developed as part of our IMO <a href="https://deepmind.google/discover/blog/advanced-version-of-gemini-with-deep-think-officially-achieves-gold-medal-standard-at-the-international-mathematical-olympiad/">effort</a>. On this larger pool, the automatic grader achieved a lower, but still reasonable Pearson correlation coefficient of 0.87.</p>

<div style="text-align: center;">
  <img src="imgs/proofgrader_scatterplot.png" alt="Correlation between human expert grades and ProofAutoGrader scores">
</div>

<h2>IMO-GradingBench</h2>

<p>IMO-GradingBench, with 1000 human gradings on proofs, is designed to enable further progress in automatic evaluation of long-form answers.</p>

<div style="text-align: center;">
  <img src="imgs/imo_grading_bench.png" alt="Distribution of human grades in IMO-GradingBench" height="500">
</div>

<div id="citation-container">
	<pre id="citation-code-block">
		<button class="copy-citation-button" onclick="copyCitation()">
			<i class="ph ph-copy"></i>
			<span id="copy-text">Copy BibTeX</span>
		</button>
		<code>
@inproceedings{luong-etal-2025-towards,
title = "Towards Robust Mathematical Reasoning",
author  = {Thang Luong and Dawsen Hwang and Hoang H. Nguyen and Golnaz Ghiasi and Yuri Chervonyi and Insuk Seo and Junsu Kim and Garrett Bingham and Jonathan Lee and Swaroop Mishra and Alex Zhai and Clara Huiyi Hu and Henryk Michalewski and Jimin Kim and Jeonghyun Ahn and Junhwi Bae and Xingyou Song and Trieu H. Trinh and Quoc V. Le and Junehyuk Jung},
booktitle = "Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing",
year = "2025",
url = "https://aclanthology.org/2025.emnlp-main.1794/",
}
		</code>
	</pre>
</div>

<script>
    /**
     * Copies the text content of the code block to the clipboard.
     * Uses execCommand('copy') as a robust fallback for iframe environments.
     */
    function copyCitation() {
        const citationBlock = document.getElementById('citation-code-block').querySelector('code');
        const button = document.querySelector('.copy-citation-button');
        const copyTextSpan = document.getElementById('copy-text');
        
        // 1. Get the raw text content
        const textToCopy = citationBlock.innerText.trim();

        // 2. Use document.execCommand('copy') as it works reliably in sandboxed environments (like iframes)
        const textarea = document.createElement('textarea');
        textarea.value = textToCopy;
        textarea.style.position = 'fixed'; // Prevents scrolling to the bottom
        document.body.appendChild(textarea);
        
        textarea.select();
        
        let success = false;
        try {
            success = document.execCommand('copy');
        } catch (err) {
            console.error('Failed to copy text using execCommand', err);
        }

        document.body.removeChild(textarea);

        // 3. Provide user feedback
        if (success) {
            button.classList.add('copied');
            copyTextSpan.textContent = 'Copied!';
            button.querySelector('i').className = 'ph ph-check-circle';

            setTimeout(() => {
                button.classList.remove('copied');
                copyTextSpan.textContent = 'Copy BibTeX';
                button.querySelector('i').className = 'ph ph-copy';
            }, 2000);
        } else {
            // Optional: Fallback notification if copy fails
            copyTextSpan.textContent = 'Copy Failed!';
            setTimeout(() => {
                copyTextSpan.textContent = 'Copy BibTeX';
            }, 2000);
        }
    }
</script>

</body>

